{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "95fc30d0",
   "metadata": {},
   "source": [
    "# Sailors"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "29c2340b",
   "metadata": {},
   "source": [
    "### Requirements"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "f693a08a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import os\n",
    "import numpy as np\n",
    "\n",
    "# Add the src directory to the path\n",
    "sys.path.append(os.path.abspath('../src'))\n",
    "sys.path.append(os.path.abspath('..'))\n",
    "\n",
    "# Import the BaseAgent class\n",
    "from src.agents.base_agent import BaseAgent\n",
    "\n",
    "from src.env_sailing import SailingEnv\n",
    "from src.initial_windfields import get_initial_windfield\n",
    "\n",
    "# Display the BaseAgent class documentation\n",
    "#help(BaseAgent)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "117050e1",
   "metadata": {},
   "source": [
    "### Training function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "265b8f11",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_agent(\n",
    "        agent: BaseAgent,\n",
    "        seed: int = 42,\n",
    "        env_version: str = \"simple_static\",\n",
    "        num_episodes = 100,\n",
    "        max_steps = 1000,\n",
    "        verbose = True\n",
    "        ):\n",
    "\n",
    "    # Set fixed seed for reproducibility\n",
    "    agent.seed(42)\n",
    "\n",
    "    # Create environment with a simple initial windfield\n",
    "    if env_version in ['simple_static']:\n",
    "        env = SailingEnv(**get_initial_windfield('simple_static'))\n",
    "    else:\n",
    "        raise ValueError \n",
    "    \n",
    "    print(\"Starting training...\")\n",
    "\n",
    "    for episode in range(num_episodes):\n",
    "        # Reset environment and get initial state\n",
    "        observation, info = env.reset(seed=episode)  # Different seed each episode\n",
    "        state = agent.discretize_state(observation)\n",
    "        \n",
    "    # Progress tracking\n",
    "    rewards_history = []\n",
    "    steps_history = []\n",
    "    success_history = []\n",
    "\n",
    "    # Training loop\n",
    "    print(\"Starting full training with 100 episodes...\")\n",
    "    import time\n",
    "    start_time = time.time()\n",
    "\n",
    "    for episode in range(num_episodes):\n",
    "        # Reset environment and get initial state\n",
    "        observation, info = env.reset(seed=episode)  # Different seed each episode\n",
    "        state = agent.discretize_state(observation)\n",
    "        \n",
    "        total_reward = 0\n",
    "        \n",
    "        for step in range(max_steps):\n",
    "            # Select action and take step\n",
    "            action = agent.act(observation)\n",
    "            next_observation, reward, done, truncated, info = env.step(action)\n",
    "            next_state = agent.discretize_state(next_observation)\n",
    "            \n",
    "            # Update Q-table\n",
    "            agent.learn(state, action, reward, next_state)\n",
    "            \n",
    "            # Update state and total reward\n",
    "            state = next_state\n",
    "            observation = next_observation\n",
    "            total_reward += reward\n",
    "            \n",
    "            # Break if episode is done\n",
    "            if done or truncated:\n",
    "                break\n",
    "        \n",
    "        # Record metrics\n",
    "        rewards_history.append(total_reward)\n",
    "        steps_history.append(step+1)\n",
    "        success_history.append(done)\n",
    "        \n",
    "        # Update exploration rate (decrease over time)\n",
    "        agent.exploration_rate = max(0.05, agent.exploration_rate * 0.98)\n",
    "        \n",
    "        # Print progress every 10 episodes\n",
    "        if (episode + 1) % 10 == 0:\n",
    "            success_rate = sum(success_history[-10:]) / 10 * 100\n",
    "            print(f\"Episode {episode+1}/100: Success rate (last 10): {success_rate:.1f}%\")\n",
    "\n",
    "    training_time = time.time() - start_time\n",
    "\n",
    "    # Calculate overall success rate\n",
    "    success_rate = sum(success_history) / len(success_history) * 100\n",
    "\n",
    "    print(f\"\\nTraining completed in {training_time:.1f} seconds!\")\n",
    "    print(f\"Success rate: {success_rate:.1f}%\")\n",
    "    print(f\"Average reward: {np.mean(rewards_history):.2f}\")\n",
    "    print(f\"Average steps: {np.mean(steps_history):.1f}\")\n",
    "    print(f\"Q-table size: {len(agent.q_table)} states\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "262ce54d",
   "metadata": {},
   "source": [
    "## I. Mousse_1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0b0bf910",
   "metadata": {},
   "source": [
    "### Defining Agent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "8fbe884c",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Mousse_1(BaseAgent):\n",
    "    \"\"\"A minimal valid agent that meets all interface requirements.\"\"\"\n",
    "    \n",
    "    def __init__(self, learning_rate=0.1, discount_factor=0.9, exploration_rate=0.1):\n",
    "        super().__init__()\n",
    "        self.np_random = np.random.default_rng()\n",
    "\n",
    "        self.learning_rate = learning_rate\n",
    "        self.discount_factor = discount_factor\n",
    "        self.exploration_rate = exploration_rate\n",
    "\n",
    "        self.position_bins = 8     # Discretize the grid into 8x8\n",
    "        self.velocity_bins = 4     # Discretize velocity into 4 bins\n",
    "        self.wind_bins = 8         # Discretize wind directions into 8 bins\n",
    "\n",
    "        self.q_table = {}\n",
    "\n",
    "    def discretize_state(self, observation):\n",
    "        \"\"\"Convert continuous observation to discrete state for Q-table lookup.\"\"\"\n",
    "        # Extract position, velocity and wind from observation\n",
    "        x, y = observation[0], observation[1]\n",
    "        vx, vy = observation[2], observation[3]\n",
    "        wx, wy = observation[4], observation[5]\n",
    "        \n",
    "        # Discretize position (assume 32x32 grid)\n",
    "        grid_size = 32\n",
    "        x_bin = min(int(x / grid_size * self.position_bins), self.position_bins - 1)\n",
    "        y_bin = min(int(y / grid_size * self.position_bins), self.position_bins - 1)\n",
    "        \n",
    "        # Discretize velocity direction (ignoring magnitude for simplicity)\n",
    "        v_magnitude = np.sqrt(vx**2 + vy**2)\n",
    "        if v_magnitude < 0.1:  # If velocity is very small, consider it as a separate bin\n",
    "            v_bin = 0\n",
    "        else:\n",
    "            v_direction = np.arctan2(vy, vx)  # Range: [-pi, pi]\n",
    "            v_bin = int(((v_direction + np.pi) / (2 * np.pi) * (self.velocity_bins-1)) + 1) % self.velocity_bins\n",
    "        \n",
    "        # Discretize wind direction\n",
    "        wind_direction = np.arctan2(wy, wx)  # Range: [-pi, pi]\n",
    "        wind_bin = int(((wind_direction + np.pi) / (2 * np.pi) * self.wind_bins)) % self.wind_bins\n",
    "        \n",
    "        # Return discrete state tuple\n",
    "        return (x_bin, y_bin, v_bin, wind_bin)\n",
    "    \n",
    "    def act(self, observation: np.ndarray) -> int:\n",
    "        \"\"\"Choose an action using epsilon-greedy policy.\"\"\"\n",
    "        # Discretize the state\n",
    "        state = self.discretize_state(observation)\n",
    "        \n",
    "        # Epsilon-greedy action selection\n",
    "        if self.np_random.random() < self.exploration_rate:\n",
    "            # Explore: choose a random action\n",
    "            return self.np_random.integers(0, 9)\n",
    "        else:\n",
    "            # Exploit: choose the best action according to Q-table\n",
    "            if state not in self.q_table:\n",
    "                # If state not in Q-table, initialize it\n",
    "                self.q_table[state] = np.zeros(9)\n",
    "            \n",
    "            # Return action with highest Q-value\n",
    "            return np.argmax(self.q_table[state])\n",
    "        \n",
    "\n",
    "    def learn(self, state, action, reward, next_state):\n",
    "        \"\"\"Update Q-table based on observed transition.\"\"\"\n",
    "        # Initialize Q-values if states not in table\n",
    "        if state not in self.q_table:\n",
    "            self.q_table[state] = np.zeros(9)\n",
    "        if next_state not in self.q_table:\n",
    "            self.q_table[next_state] = np.zeros(9)\n",
    "        \n",
    "        # Q-learning update\n",
    "        best_next_action = np.argmax(self.q_table[next_state])\n",
    "        td_target = reward + self.discount_factor * self.q_table[next_state][best_next_action]\n",
    "        td_error = td_target - self.q_table[state][action]\n",
    "        self.q_table[state][action] += self.learning_rate * td_error\n",
    "    \n",
    "    def reset(self) -> None:\n",
    "        \"\"\"Reset the agent.\"\"\"\n",
    "        pass  # Nothing to reset in this simple agent\n",
    "    \n",
    "    def seed(self, seed: int = None) -> None:\n",
    "        \"\"\"Set the random seed.\"\"\"\n",
    "        self.np_random = np.random.default_rng(seed)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e67a8792",
   "metadata": {},
   "source": [
    "### Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "edecb557",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting training with 10 episodes (debug run)...\n",
      "Starting full training with 100 episodes...\n",
      "Episode 10/100: Success rate (last 10): 100.0%\n",
      "Episode 20/100: Success rate (last 10): 100.0%\n",
      "Episode 30/100: Success rate (last 10): 100.0%\n",
      "Episode 40/100: Success rate (last 10): 100.0%\n",
      "Episode 50/100: Success rate (last 10): 100.0%\n",
      "Episode 60/100: Success rate (last 10): 100.0%\n",
      "Episode 70/100: Success rate (last 10): 100.0%\n",
      "Episode 80/100: Success rate (last 10): 100.0%\n",
      "Episode 90/100: Success rate (last 10): 100.0%\n",
      "Episode 100/100: Success rate (last 10): 100.0%\n",
      "Episode 110/100: Success rate (last 10): 100.0%\n",
      "Episode 120/100: Success rate (last 10): 100.0%\n",
      "Episode 130/100: Success rate (last 10): 100.0%\n",
      "Episode 140/100: Success rate (last 10): 100.0%\n",
      "Episode 150/100: Success rate (last 10): 100.0%\n",
      "Episode 160/100: Success rate (last 10): 100.0%\n",
      "Episode 170/100: Success rate (last 10): 100.0%\n",
      "Episode 180/100: Success rate (last 10): 100.0%\n",
      "Episode 190/100: Success rate (last 10): 100.0%\n",
      "Episode 200/100: Success rate (last 10): 100.0%\n",
      "\n",
      "Training completed in 2.9 seconds!\n",
      "Success rate: 100.0%\n",
      "Average reward: 79.50\n",
      "Average steps: 524.8\n",
      "Q-table size: 294 states\n"
     ]
    }
   ],
   "source": [
    "mousse_1 = Mousse_1(learning_rate=0.1, discount_factor=0.99, exploration_rate=0.2)\n",
    "\n",
    "# Set fixed seed for reproducibility\n",
    "np.random.seed(42)\n",
    "mousse_1.seed(42)\n",
    "\n",
    "# Create environment with a simple initial windfield\n",
    "env = SailingEnv(**get_initial_windfield('simple_static'))\n",
    "\n",
    "\n",
    "\n",
    "# Training parameters\n",
    "num_episodes = 200  # Small number for debugging\n",
    "max_steps = 1000\n",
    "\n",
    "\n",
    "print(\"Starting training with 10 episodes (debug run)...\")\n",
    "for episode in range(num_episodes):\n",
    "    # Reset environment and get initial state\n",
    "    observation, info = env.reset(seed=episode)  # Different seed each episode\n",
    "    state = mousse_1.discretize_state(observation)\n",
    "    \n",
    "# Progress tracking\n",
    "rewards_history = []\n",
    "steps_history = []\n",
    "success_history = []\n",
    "\n",
    "# Training loop\n",
    "print(\"Starting full training with 100 episodes...\")\n",
    "import time\n",
    "start_time = time.time()\n",
    "\n",
    "for episode in range(num_episodes):\n",
    "    # Reset environment and get initial state\n",
    "    observation, info = env.reset(seed=episode)  # Different seed each episode\n",
    "    state = mousse_1.discretize_state(observation)\n",
    "    \n",
    "    total_reward = 0\n",
    "    \n",
    "    for step in range(max_steps):\n",
    "        # Select action and take step\n",
    "        action = mousse_1.act(observation)\n",
    "        next_observation, reward, done, truncated, info = env.step(action)\n",
    "        next_state = mousse_1.discretize_state(next_observation)\n",
    "        \n",
    "        # Update Q-table\n",
    "        mousse_1.learn(state, action, reward, next_state)\n",
    "        \n",
    "        # Update state and total reward\n",
    "        state = next_state\n",
    "        observation = next_observation\n",
    "        total_reward += reward\n",
    "        \n",
    "        # Break if episode is done\n",
    "        if done or truncated:\n",
    "            break\n",
    "    \n",
    "    # Record metrics\n",
    "    rewards_history.append(total_reward)\n",
    "    steps_history.append(step+1)\n",
    "    success_history.append(done)\n",
    "    \n",
    "    # Update exploration rate (decrease over time)\n",
    "    mousse_1.exploration_rate = max(0.05, mousse_1.exploration_rate * 0.98)\n",
    "    \n",
    "    # Print progress every 10 episodes\n",
    "    if (episode + 1) % 10 == 0:\n",
    "        success_rate = sum(success_history[-10:]) / 10 * 100\n",
    "        print(f\"Episode {episode+1}/100: Success rate (last 10): {success_rate:.1f}%\")\n",
    "\n",
    "training_time = time.time() - start_time\n",
    "\n",
    "# Calculate overall success rate\n",
    "success_rate = sum(success_history) / len(success_history) * 100\n",
    "\n",
    "print(f\"\\nTraining completed in {training_time:.1f} seconds!\")\n",
    "print(f\"Success rate: {success_rate:.1f}%\")\n",
    "print(f\"Average reward: {np.mean(rewards_history):.2f}\")\n",
    "print(f\"Average steps: {np.mean(steps_history):.1f}\")\n",
    "print(f\"Q-table size: {len(mousse_1.q_table)} states\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "92082755",
   "metadata": {},
   "source": [
    "### Testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "17b7b805",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Testing the trained agent on 5 new episodes...\n",
      "Test Episode 1: Steps=297, Reward=100.0, Position=[15 31], Goal reached=True\n",
      "Test Episode 2: Steps=328, Reward=100.0, Position=[15 30], Goal reached=True\n",
      "Test Episode 3: Steps=97, Reward=100.0, Position=[15 30], Goal reached=True\n",
      "Test Episode 4: Steps=141, Reward=100.0, Position=[15 30], Goal reached=True\n",
      "Test Episode 5: Steps=145, Reward=100.0, Position=[15 30], Goal reached=True\n"
     ]
    }
   ],
   "source": [
    "# Turn off exploration for evaluation\n",
    "mousse_1.exploration_rate = 0\n",
    "\n",
    "# Create test environment\n",
    "test_env = SailingEnv(**get_initial_windfield('training_1'))\n",
    "\n",
    "# Test parameters\n",
    "num_test_episodes = 5\n",
    "max_steps = 1000\n",
    "\n",
    "print(\"Testing the trained agent on 5 new episodes...\")\n",
    "# Testing loop\n",
    "for episode in range(num_test_episodes):\n",
    "    # Reset environment\n",
    "    observation, info = test_env.reset(seed=1000 + episode)  # Different seeds from training\n",
    "    \n",
    "    total_reward = 0\n",
    "    \n",
    "    for step in range(max_steps):\n",
    "        # Select action using learned policy\n",
    "        action = mousse_1.act(observation)\n",
    "        observation, reward, done, truncated, info = test_env.step(action)\n",
    "        \n",
    "        total_reward += reward\n",
    "        \n",
    "        # Break if episode is done\n",
    "        if done or truncated:\n",
    "            break\n",
    "    \n",
    "    print(f\"Test Episode {episode+1}: Steps={step+1}, Reward={total_reward}, \" +\n",
    "          f\"Position={info['position']}, Goal reached={done}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "sail_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
