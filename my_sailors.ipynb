{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "95fc30d0",
   "metadata": {},
   "source": [
    "# Sailors"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "29c2340b",
   "metadata": {},
   "source": [
    "### Requirements"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f693a08a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import os\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Add the src directory to the path\n",
    "sys.path.append(os.path.abspath('../src'))\n",
    "sys.path.append(os.path.abspath('..'))\n",
    "\n",
    "# Import the BaseAgent class\n",
    "from src.agents.base_agent import BaseAgent\n",
    "\n",
    "from src.env_sailing import SailingEnv\n",
    "from src.initial_windfields import get_initial_windfield\n",
    "\n",
    "# Display the BaseAgent class documentation\n",
    "#help(BaseAgent)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "262ce54d",
   "metadata": {},
   "source": [
    "## I. Mousse_1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0b0bf910",
   "metadata": {},
   "source": [
    "### Defining Agent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8fbe884c",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Mousse_1(BaseAgent):\n",
    "    \"\"\"A minimal valid agent that meets all interface requirements.\"\"\"\n",
    "    \n",
    "    def __init__(self, learning_rate=0.1, discount_factor=0.9, exploration_rate=0.1):\n",
    "        super().__init__()\n",
    "        self.np_random = np.random.default_rng()\n",
    "\n",
    "        self.learning_rate = learning_rate\n",
    "        self.discount_factor = discount_factor\n",
    "        self.exploration_rate = exploration_rate\n",
    "\n",
    "        self.position_bins = 32     # Discretize the grid into 32 x 32\n",
    "        self.velocity_bins = 8     # Discretize velocity into 4 bins\n",
    "        self.wind_bins = 8         # Discretize wind directions into 8 bins\n",
    "\n",
    "        self.q_table = {}\n",
    "\n",
    "        self.complexity = self.position_bins * self.position_bins * self.velocity_bins * self.wind_bins\n",
    "        self.q_visit = {}\n",
    "        self.ite_learned = 0\n",
    "\n",
    "        self.hist_pos = []\n",
    "        self.hist_veloc = []\n",
    "        self.hist_wind = []\n",
    "\n",
    "    def discretize_state(self, observation):\n",
    "        \"\"\"Convert continuous observation to discrete state for Q-table lookup.\"\"\"\n",
    "        # Extract position, velocity and wind from observation\n",
    "        x, y = observation[0], observation[1]\n",
    "        vx, vy = observation[2], observation[3]\n",
    "        wx, wy = observation[4], observation[5]\n",
    "        \n",
    "        # Discretize position (assume 32x32 grid)\n",
    "        grid_size = 32\n",
    "        x_bin = min(int(x / grid_size * self.position_bins), self.position_bins - 1)\n",
    "        y_bin = min(int(y / grid_size * self.position_bins), self.position_bins - 1)\n",
    "        \n",
    "        # Discretize velocity direction (ignoring magnitude for simplicity)\n",
    "        v_magnitude = np.sqrt(vx**2 + vy**2)\n",
    "        if v_magnitude < 0.1:  # If velocity is very small, consider it as a separate bin\n",
    "            v_bin = 0\n",
    "        else:\n",
    "            v_direction = np.arctan2(vy, vx)  # Range: [-pi, pi]\n",
    "            v_bin = int(((v_direction + np.pi) / (2 * np.pi) * (self.velocity_bins-1)) + 1) % self.velocity_bins\n",
    "        \n",
    "        # Discretize wind direction\n",
    "        wind_direction = np.arctan2(wy, wx)  # Range: [-pi, pi]\n",
    "        wind_bin = int(((wind_direction + np.pi) / (2 * np.pi) * self.wind_bins)) % self.wind_bins\n",
    "\n",
    "        # Return discrete state tuple\n",
    "        return (x_bin, y_bin, v_bin, wind_bin)\n",
    "    \n",
    "    def act(self, observation: np.ndarray) -> int:\n",
    "        \"\"\"Choose an action using epsilon-greedy policy.\"\"\"\n",
    "        # Discretize the state\n",
    "        state = self.discretize_state(observation)\n",
    "        self.hist_pos.append((state[0], state[1]))\n",
    "        self.hist_veloc.append((state[2], state[3]))\n",
    "        \n",
    "        # Epsilon-greedy action selection\n",
    "        if self.np_random.random() < self.exploration_rate:\n",
    "            # Explore: choose a random action\n",
    "            return self.np_random.integers(0, 9)\n",
    "        else:\n",
    "            # Exploit: choose the best action according to Q-table\n",
    "            if state not in self.q_table:\n",
    "                # If state not in Q-table, initialize it\n",
    "                self.q_table[state] = np.zeros(9)\n",
    "            \n",
    "            # Return action with highest Q-value\n",
    "            return np.argmax(self.q_table[state])\n",
    "        \n",
    "\n",
    "    def learn(self, state, action, reward, next_state):\n",
    "        \"\"\"Update Q-table based on observed transition.\"\"\"\n",
    "        self.ite_learned += 1\n",
    "\n",
    "        # Initialize Q-values if states not in table\n",
    "        if state not in self.q_table:\n",
    "            self.q_table[state] = np.zeros(9)\n",
    "        if next_state not in self.q_table:\n",
    "            self.q_table[next_state] = np.zeros(9)\n",
    "            \n",
    "        if state not in self.q_visit:    \n",
    "            self.q_visit[state] = 0\n",
    "        else:\n",
    "            self.q_visit[state] += 1\n",
    "        \n",
    "        # Q-learning update\n",
    "        best_next_action = np.argmax(self.q_table[next_state])\n",
    "        td_target = reward + self.discount_factor * self.q_table[next_state][best_next_action]\n",
    "        td_error = td_target - self.q_table[state][action]\n",
    "        self.q_table[state][action] += self.learning_rate * td_error\n",
    "    \n",
    "    def reset(self) -> None:\n",
    "        \"\"\"Reset the agent.\"\"\"\n",
    "        self.q_table.clear()\n",
    "        self.ite_learned = 0\n",
    "    \n",
    "    def set_hist(self) -> None:\n",
    "        \"\"\"Reset only history variables\"\"\"\n",
    "        self.hist_pos.clear()\n",
    "        self.hist_veloc.clear()\n",
    "        self.hist_wind.clear()\n",
    "\n",
    "\n",
    "    def seed(self, seed: int = None) -> dict:\n",
    "        \"\"\"Set the random seed.\"\"\"\n",
    "        self.np_random = np.random.default_rng(seed)\n",
    "\n",
    "    def show_visited_state(self, wind_: bool = False, interpol = \"nearest\", show = True):\n",
    "        dico_mat = {}\n",
    "        l = 1\n",
    "        if wind_:\n",
    "            l = self.wind_bins\n",
    "\n",
    "        for i in range(l):\n",
    "                dico_mat[i] = np.zeros((self.position_bins, self.position_bins))\n",
    "\n",
    "        for key in self.q_visit.keys():\n",
    "            x, y, velocity, wind, dist = key\n",
    "            if wind_ == False:\n",
    "                wind = 0\n",
    "            dico_mat[wind][y][x] += self.q_visit[key] / self.ite_learned\n",
    "\n",
    "\n",
    "        if show == True:\n",
    "            for i in range(l):\n",
    "                #dico_mat[i] = np.log(dico_mat[i] + 1)\n",
    "                plt.figure(figsize=(6, 6))\n",
    "                plt.imshow(dico_mat[i], cmap='Blues', interpolation=interpol)\n",
    "                plt.colorbar()\n",
    "                plt.title(label=f\"Wind direction {i}\")\n",
    "                plt.gca().invert_yaxis()  # Inversion de l'axe Y\n",
    "                plt.show()\n",
    "        else:\n",
    "            return dico_mat\n",
    "    \n",
    "    def show_velocity(self, wind_: bool = True, interpol = \"nearest\", show=True):\n",
    "        dico_visit = self.show_visited_state(wind_= wind_, show = False)\n",
    "        dico_mat = {}\n",
    "        l = 1\n",
    "        if wind_:\n",
    "            l = self.wind_bins\n",
    "\n",
    "        for i in range(l):\n",
    "                dico_mat[i] = np.zeros((self.position_bins, self.position_bins))\n",
    "\n",
    "        for key in self.q_visit.keys():\n",
    "            x, y, velocity, wind, dist = key\n",
    "            if wind_ == False:\n",
    "                wind = 0\n",
    "            dico_mat[wind][y][x] += velocity / dico_visit[wind][y][x]\n",
    "\n",
    "        if show==True:\n",
    "            for i in range(l):\n",
    "                plt.figure(figsize=(6, 6))\n",
    "                plt.imshow(dico_mat[i], cmap='Blues', interpolation=interpol)\n",
    "                plt.colorbar()\n",
    "                plt.title(label=f\"Wind direction {i}\")\n",
    "                plt.gca().invert_yaxis()  # Inversion de l'axe Y\n",
    "                plt.show()\n",
    "        else:\n",
    "            return dico_mat\n",
    "        \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e67a8792",
   "metadata": {},
   "source": [
    "### Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "089debbe",
   "metadata": {},
   "outputs": [],
   "source": [
    "sailor = Mousse_1(learning_rate=0.1, discount_factor=0.99, exploration_rate=0.3)\n",
    "\n",
    "np.random.seed(42)\n",
    "sailor.seed(42)\n",
    "\n",
    "still_training = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "edecb557",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training parameters\n",
    "sailor.learning_rate = 0.1\n",
    "sailor.discount_factor = 0.99\n",
    "sailor.exploration_rate = 0.3\n",
    "\n",
    "num_episodes = 60000  # Small number for debugging\n",
    "max_steps = 1000    \n",
    "\n",
    "batch = 100\n",
    "batch_mat = 20000\n",
    "\n",
    "\n",
    "if still_training == False:\n",
    "# This line reinitializes q table\n",
    "    sailor.reset()\n",
    "\n",
    "# Create environment with a simple initial windfield\n",
    "dico_env = {}\n",
    "dico_env[1] = SailingEnv(**get_initial_windfield('training_1'))\n",
    "dico_env[2] = SailingEnv(**get_initial_windfield('training_2'))\n",
    "dico_env[3] = SailingEnv(**get_initial_windfield('training_3'))\n",
    "    \n",
    "# Progress tracking\n",
    "rewards_history = []\n",
    "steps_history = []\n",
    "success_history = []\n",
    "true_rew = []\n",
    "\n",
    "# Training loop\n",
    "print(f\"Starting full training with {num_episodes} episodes...\")\n",
    "import time\n",
    "start_time = time.time()\n",
    "\n",
    "for episode in range(num_episodes):\n",
    "    i = np.random.randint(1, 4)\n",
    "    env = dico_env[i]\n",
    "    # Reset environment and get initial state\n",
    "    sailor.set_hist()\n",
    "\n",
    "    observation, info = env.reset(seed=episode)  # Different seed each episode\n",
    "    state = sailor.discretize_state(observation)\n",
    "    \n",
    "    total_reward = 0\n",
    "    \n",
    "    for step in range(max_steps):\n",
    "        # Select action and take step\n",
    "        action = sailor.act(observation)\n",
    "        next_observation, reward, done, truncated, info = env.step(action)\n",
    "        next_state = sailor.discretize_state(next_observation)\n",
    "        \n",
    "        # Update Q-table\n",
    "        sailor.learn(state, action, reward, next_state)\n",
    "        \n",
    "        # Update state and total reward\n",
    "        state = next_state\n",
    "        observation = next_observation\n",
    "        total_reward += reward\n",
    "        \n",
    "        # Break if episode is done\n",
    "        if done or truncated:\n",
    "            break\n",
    "    \n",
    "    # Record metrics\n",
    "    rewards_history.append(total_reward)\n",
    "    steps_history.append(step+1)\n",
    "    success_history.append(done)\n",
    "    true_rew.append(np.power(sailor.discount_factor,step)*total_reward)\n",
    "\n",
    "    # Update exploration rate (decrease over time)\n",
    "    if (episode + 1) % batch_mat == 0:\n",
    "        sailor.exploration_rate = 0.3\n",
    "    else :\n",
    "        sailor.exploration_rate = max(0.05, sailor.exploration_rate * 0.999)\n",
    "    \n",
    "    # Print progress every 10 episodes\n",
    "\n",
    "    # Plot progress\n",
    "\n",
    "    if (episode + 1) % batch == 0:\n",
    "        step_avg = sum(steps_history[-batch:]) / batch\n",
    "        print(f\"Episode {episode+1}/{num_episodes}: Steps avg: {step_avg} - \" +\n",
    "              f\"exp rate: {round(sailor.exploration_rate,4)} - q size: {len(sailor.q_table)}\"+\n",
    "               f\" ({round(len(sailor.q_table)/sailor.complexity,2)})         \", end=\"\\r\")\n",
    "\n",
    "    if (episode + 1) % batch_mat == 0:\n",
    "        sailor.show_visited_state(wind_=False,show=True)\n",
    "\n",
    "training_time = time.time() - start_time\n",
    "\n",
    "# Calculate overall success rate\n",
    "success_rate = sum(success_history) / len(success_history) * 100\n",
    "\n",
    "print(f\"\\nTraining completed in {training_time:.1f} seconds!\")\n",
    "print(f\"Success rate: {success_rate:.1f}%\")\n",
    "print(f\"Average reward: {np.mean(rewards_history):.2f}\")\n",
    "print(f\"Average steps: {np.mean(steps_history):.1f}\")\n",
    "print(f\"Q-table size: {len(sailor.q_table)} states\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "93765abf",
   "metadata": {},
   "source": [
    "### Training Performance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4c61439f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Calculate rolling averages\n",
    "window_size = 10\n",
    "rolling_rewards = np.convolve(true_rew, np.ones(window_size)/window_size, mode='valid')\n",
    "rolling_steps = np.convolve(steps_history, np.ones(window_size)/window_size, mode='valid')\n",
    "rolling_success = np.convolve([1 if s else 0 for s in success_history], np.ones(window_size)/window_size, mode='valid') * 100\n",
    "\n",
    "# Create the plots\n",
    "# fig, (ax1, ax2, ax3) = plt.subplots(3, 1, figsize=(10, 12), sharex=True)\n",
    "fig, (ax1, ax2) = plt.subplots(2, 1, figsize=(10, 12), sharex=True)\n",
    "\n",
    "# Plot rewards\n",
    "ax1.plot(rolling_rewards)\n",
    "ax1.set_ylabel('Average Reward')\n",
    "ax1.set_title('Training Progress (10-episode rolling average)')\n",
    "\n",
    "# Plot steps\n",
    "ax2.plot(rolling_steps)\n",
    "ax2.set_ylabel('Average Steps')\n",
    "\n",
    "# Plot success rate\n",
    "#ax3.plot(rolling_success)\n",
    "#ax3.set_ylabel('Success Rate (%)')\n",
    "#ax3.set_xlabel('Episode')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "92082755",
   "metadata": {},
   "source": [
    "### Testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "17b7b805",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Turn off exploration for evaluation\n",
    "sailor.exploration_rate = 0\n",
    "\n",
    "# Create test environment\n",
    "test_env = SailingEnv(**get_initial_windfield('training_1'))\n",
    "\n",
    "# Test parameters\n",
    "num_test_episodes = 5\n",
    "max_steps = 1000\n",
    "\n",
    "print(\"Testing the trained agent on 5 new episodes...\")\n",
    "# Testing loop\n",
    "for episode in range(num_test_episodes):\n",
    "    # Reset environment\n",
    "    observation, info = test_env.reset(seed=1000 + episode)  # Different seeds from training\n",
    "    distance = 32\n",
    "    sailor.set_hist()\n",
    "    \n",
    "    total_reward = 0\n",
    "    \n",
    "    for step in range(max_steps):\n",
    "        # Select action using learned policy\n",
    "        action = sailor.act(observation, distance=distance)\n",
    "        observation, reward, done, truncated, info = test_env.step(action)\n",
    "        distance = info['distance_to_goal']\n",
    "        \n",
    "        total_reward += reward\n",
    "        \n",
    "        # Break if episode is done\n",
    "        if done or truncated:\n",
    "            break\n",
    "    \n",
    "    print(f\"Test Episode {episode+1}: Steps={step+1}, Reward={total_reward}, \" +\n",
    "          f\"Position={info['position']}, Goal reached={done}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "25fb1321",
   "metadata": {},
   "source": [
    "### Saving Agent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b06bfe76",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import the utility function for saving Q-learning agents\n",
    "from src.utils.agent_utils import save_qlearning_agent\n",
    "\n",
    "# Save our trained agent\n",
    "save_qlearning_agent(\n",
    "    agent=sailor,\n",
    "    output_path=\"agents/mousse_1.py\",\n",
    "    agent_class_name=\"Mousse_1\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eba8058e",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "sail_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
